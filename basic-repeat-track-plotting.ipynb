{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1d0a8-3b4b-4f89-8211-da527d67ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"GDAL_DATA\"] = \"/srv/conda/envs/notebook/share/gdal\" # need to specify to make gdal work\n",
    "os.environ[\"PROJ_LIB\"] = \"/srv/conda/envs/notebook/share/proj\" # need to specify to make pyproj work\n",
    "os.environ[\"PROJ_DATA\"] = \"/srv/conda/envs/notebook/share/proj\" # need to specify to make pyproj work\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm \n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import geopandas as gpd\n",
    "from ipyleaflet import Map, basemaps, Polygon, Polyline, GeoData, LayersControl\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from rasterio import warp\n",
    "import icepyx as ipx\n",
    "import shutil\n",
    "\n",
    "if 'cmcrameri' not in sys.modules:\n",
    "    ! pip3 install cmcrameri >/dev/null\n",
    "if 'contextily' not in sys.modules:\n",
    "    ! pip3 install contextily >/dev/null\n",
    "import cmcrameri.cm as cmc\n",
    "import contextily as cx\n",
    "\n",
    "try:\n",
    "    from ed.edcreds import getedcreds\n",
    "except:\n",
    "    print('No earthdata credentials found.')\n",
    "    print('To set up, rename the folder \"ed_example\" to \"ed\".')\n",
    "    print('Then, add your earthdata credentials to \"ed/edcreds.py\".')\n",
    "\n",
    "from utils.nsidc import download_is2\n",
    "from utils.readers import read_atl06\n",
    "from utils.S2 import plotS2cloudfree, add_inset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d861dc4-b4a3-47ae-b05b-c8cb276f059f",
   "metadata": {},
   "source": [
    "## Show area of interest on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69daa2d-89b2-4bbb-a25b-5343cd142653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I just drew this randomly on a map...\n",
    "shape = 'data/shapefiles/amery_gillock_kreitzer.shp'\n",
    "\n",
    "#################################################\n",
    "# for testing\n",
    "# shape = 'data/shapefiles/mini_test_region.shp'\n",
    "#################################################\n",
    "\n",
    "gdf = gpd.read_file(shape)\n",
    "geom = [i for i in gdf.geometry]\n",
    "lons,lats = geom[0].exterior.coords.xy\n",
    "shape_coords = list(zip(lats,lons))\n",
    "shape_bounds = [[np.min(lats),np.min(lons)], [np.max(lats),np.max(lons)]]\n",
    "\n",
    "m=Map(basemap=basemaps.Esri.WorldImagery,center=[np.mean(lats),np.mean(lons)],zoom=7)\n",
    "m.fit_bounds(shape_bounds)\n",
    "polygon = Polygon(locations=shape_coords, color=\"red\", fill_color=\"red\", name='polygon_buffered')\n",
    "m.add_layer(polygon)\n",
    "m.add_control(LayersControl())\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca8256d-f115-4c21-8c90-1928ca654625",
   "metadata": {},
   "source": [
    "## Download the ATL06 ICESat-2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c86803-b5f4-4861-9a77-adaf72b97e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"EARTHDATA_USERNAME\"] = getedcreds()[0]\n",
    "os.environ[\"EARTHDATA_PASSWORD\"] = getedcreds()[1]\n",
    "\n",
    "short_name = 'ATL06'\n",
    "spatial_extent = shape\n",
    "temporal = ['2018-01-01','2030-01-01']\n",
    "\n",
    "#################################################\n",
    "# for testing\n",
    "# temporal = ['2020-01-01','2020-03-01']\n",
    "#################################################\n",
    "\n",
    "region_a = ipx.Query(short_name, spatial_extent, temporal)\n",
    "results = region_a.avail_granules(ids=True)\n",
    "print('Found granules:')\n",
    "results_print = np.array(results).flatten()\n",
    "for i in range(np.min((len(results_print),10))):\n",
    "    print(results_print[i])\n",
    "if len(results_print)>10:\n",
    "    print('...and %d more results.' % (len(results_print)-10))\n",
    "region_a.avail_granules(ids=True)\n",
    "output_dir = 'data/IS2/' + short_name + '_' + shape[:shape.rfind('.')].split('/')[-1]\n",
    "\n",
    "#### uncomment to download again\n",
    "region_a.download_granules(output_dir)\n",
    "print('\\nsaved files to %s' % output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797410ab-18d8-4990-b717-4f5a21ab005b",
   "metadata": {},
   "source": [
    "### Get a list of all the ATL06 data files over this region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fd5b5-d6f3-4a58-b09a-17bf709e5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_for = 'ATL06_'\n",
    "search_in = output_dir + '/'\n",
    "filelist = [search_in+f for f in os.listdir(search_in) \\\n",
    "            if os.path.isfile(os.path.join(search_in, f)) & (search_for in f) & ('.h5' in f)]\n",
    "filelist.sort()\n",
    "print('There are %i files.' % len(filelist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202bd4e9-859a-469a-8c86-e3b4ac3dd4f0",
   "metadata": {},
   "source": [
    "## get a dataframe with basic file info (for all files / all tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575babdd-64f0-40f3-b52e-09822e7157f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = pd.DataFrame({'filename': filelist})\n",
    "df_files['granule_id'] = df_files.apply(lambda x: x.filename[x.filename.find('ATL06_'):], axis=1)\n",
    "df_files['date'] = df_files.apply(lambda x: x.granule_id[6:10]+'-'+x.granule_id[10:12]+'-'+x.granule_id[12:14], axis=1)\n",
    "df_files['track'] = df_files.apply(lambda x: int(x.granule_id[21:25]), axis=1)\n",
    "\n",
    "# figure out how many files there are per track \n",
    "files_per_track = df_files.groupby('track')[['filename']].count().rename(columns={'filename': 'number_of_files'})\n",
    "\n",
    "# take only the tracks with at least min_files files \n",
    "min_files = 3\n",
    "files_per_track = files_per_track[files_per_track.number_of_files >= min_files]\n",
    "\n",
    "# show the results\n",
    "files_per_track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912f9b5-4189-4c6b-82a8-a3ab41fb9031",
   "metadata": {},
   "source": [
    "## pick a single track / GTX combination, and work with only that from here on\n",
    "Just pick a single combination to show some repeat tracks and develop code. If you are happy with your code and want to do this for more data in an automated way you can always put all your code into a loop that goes over all track/GTX combinations (if you have time for that). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19cd83-f10f-43b8-bf19-b79a496e7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can always go back to here and change this manually to a different track/GTX by changing the values below\n",
    "# then re-run all the following cells\n",
    "track = 721\n",
    "gtx = 'gt3l'\n",
    "\n",
    "# print to confirm the track/GTX selection\n",
    "print('SELECTION: track %s, %s' % (track, gtx))\n",
    "\n",
    "# Figure out which files there are for this track --> get a dataframe with file info for just the selected track\n",
    "df_thistrack = df_files[df_files.track == track].copy()\n",
    "# sort by date and re-set index \n",
    "df_thistrack = df_thistrack.sort_values(by='date').reset_index(drop=True)\n",
    "# store the number of files for this track (this is the number of repeat passes / distinct dates from which we have data)\n",
    "nfiles = len(df_thistrack)\n",
    "print('  number of files available:', nfiles)\n",
    "\n",
    "# initialize data dictionary for all the data for this track/GTX combination\n",
    "gtx_data_dict = {}\n",
    "\n",
    "# LOOP FOR ALL FILES\n",
    "for i in range(nfiles):\n",
    "\n",
    "    # get the filename\n",
    "    this_filename = df_thistrack.iloc[i].filename\n",
    "    print('  - %d: %s ' % (i, this_filename))\n",
    "\n",
    "    # read in the data from this file using the function read_atl06()\n",
    "    ancillary, dfs = read_atl06(this_filename)\n",
    "\n",
    "    # make sure that there is data for the selected GTX in the file that we have read in\n",
    "    if gtx in dfs.keys():\n",
    "\n",
    "        # select the dataframe with the data for our GTX from the \"dfs\" dictionary\n",
    "        this_data = dfs[gtx]\n",
    "\n",
    "        # set data with questionable quality summary values to nan (i.e. nodata value)\n",
    "        this_data[this_data.qual_summary != 0] = np.nan\n",
    "\n",
    "        # set data with elevation values > 100 km to nan (because that's unreasonable)\n",
    "        this_data[this_data.h > 1e5] = np.nan\n",
    "\n",
    "        # now, just save the data for this gtx along with the ancillary data\n",
    "        ancillary['data'] = this_data\n",
    "        ancillary['gtx'] = gtx\n",
    "\n",
    "        # save the data to the gtx_data_dict, with the key being the date of the data acquisition\n",
    "        gtx_data_dict[ancillary['date']] = ancillary\n",
    "\n",
    "        # print a success message\n",
    "        print('    --> got %s data for %s' % (gtx, ancillary['date']))\n",
    "\n",
    "    else:\n",
    "        # if there is no data for the selected gtx on that date do nothing but print a message\n",
    "        print('    --> NO %s data available for %s' % (gtx, ancillary['date']))\n",
    "\n",
    "# figure out how many dates have data for the selected track / GTX combination and print\n",
    "number_of_dates = len(gtx_data_dict)\n",
    "print('\\nFinished reading in data for track %s, %s. There is data for %d dates.' % (track, gtx, number_of_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf1e74-198f-46e4-b0d8-d872f7232de0",
   "metadata": {},
   "source": [
    "## make a function for basic plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c7454-1f43-4c0e-a481-39bb5a8257d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_repeat_track(gtx_data_dict, track, gtx):\n",
    "    \n",
    "    # make the figure for this track / GTX selection\n",
    "    fig, ax = plt.subplots(figsize=[8,5])\n",
    "    \n",
    "    # loop through all dates\n",
    "    for date in gtx_data_dict.keys():\n",
    "    \n",
    "        # get the data and ancillary for this date\n",
    "        date_dict = gtx_data_dict[date]\n",
    "    \n",
    "        # get the strength of the beam (just an example of a property that you can access...)\n",
    "        beamstrength = date_dict['gtx_strength_dict'][gtx]\n",
    "    \n",
    "        # get the elevation data for this date (we stored this in 'data' in the cell above)\n",
    "        this_data = date_dict['data']\n",
    "    \n",
    "        # get the variables to plot (xatc and h)\n",
    "        x_along_track = (this_data.xatc - this_data.xatc.min()) / 1000\n",
    "        elevation = this_data.h\n",
    "    \n",
    "        # plot those variables\n",
    "        ax.plot(x_along_track, elevation, label='%s, %s' % (date, beamstrength))\n",
    "    \n",
    "        # set the title and axis labels, set title to track/GTX selection that we made\n",
    "        ax.set_title('track %s, %s' % (track, gtx))\n",
    "        ax.set_xlabel('along-track distance (km)')\n",
    "        ax.set_ylabel('elevation (m)')\n",
    "    \n",
    "        # make a legend\n",
    "        ax.legend(loc='upper right', fontsize=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f0699-b8a7-4474-abf5-0dc343f23342",
   "metadata": {},
   "source": [
    "## call the function to plot elevation vs along-track distance for each date available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc37e129-e194-4c7d-98ed-11693f549fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_repeat_track(gtx_data_dict, track, gtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5188389-4b01-4f14-84f4-871ec8a43cc2",
   "metadata": {},
   "source": [
    "## now that you have all the data you could make any desired changes, and then plot again!\n",
    "for example, see what happens when you run this code and then plot in the cell below again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e348ad8-d26d-4d2a-a64a-f2149a53bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the data dictionary (to keep the old one so you don't have to re-run the cells above this)\n",
    "new_gtx_data_dict = gtx_data_dict.copy()\n",
    "\n",
    "# get mean across track distances from the data (initialize empty list, then fill with values for all dates)\n",
    "mean_across_track_distances = []\n",
    "for date in new_gtx_data_dict.keys():\n",
    "    mean_across_track = np.nanmean(new_gtx_data_dict[date]['data'].yatc)\n",
    "    new_gtx_data_dict[date]['mean_across_track'] = mean_across_track\n",
    "    mean_across_track_distances.append(mean_across_track)\n",
    "\n",
    "# get the median across-track distance for all repeat passes \n",
    "across_track_median = np.nanmedian(mean_across_track_distances)\n",
    "\n",
    "# now delete the data that is further away from the median than max_difference_across_track meters\n",
    "max_difference_across_track = 10 # meters\n",
    "for date in list(new_gtx_data_dict.keys()):\n",
    "\n",
    "    # if mean across-track distance from this date is less than max_difference_across_track meters away from the median for all dates\n",
    "    across_track_offset = np.abs(new_gtx_data_dict[date]['mean_across_track'] - across_track_median)\n",
    "    keep = np.abs(across_track_offset) <= max_difference_across_track\n",
    "    action = 'keep' if keep else 'delete'\n",
    "    print('%s: across-track offset from median = %8.2f m --> %s!' % (date, across_track_offset, action))\n",
    "    \n",
    "    if not keep:\n",
    "        # delete the entry for this date\n",
    "        del new_gtx_data_dict[date]\n",
    "\n",
    "# call the plotting function again\n",
    "plot_repeat_track(new_gtx_data_dict, track, gtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c2212-ba6c-42ce-9fc2-eed176a0005a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
